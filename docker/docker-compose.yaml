version: "3.9"

services:
  fastchat-webui:
    image: fastchat:latest
    ports:
      - "80:80"
    entrypoint: [ "python3", "-m", "fastchat.serve.gradio_web_server", "--host", "0.0.0.0", "--port", "80", "--controller-url", "http://fastchat-controller:21001" ]
  fastchat-controller:
    image: fastchat:latest
    entrypoint: [ "python3", "-m", "fastchat.serve.controller", "--host", "0.0.0.0", "--port", "21001" ]
#  fastchat-model-worker:
#    volumes:
#      - huggingface:/root/.cache/huggingface
#    environment:
#      FASTCHAT_CONTROLLER_URL: http://fastchat-controller:21001
#    image: fastchat:latest
#    deploy:
#      resources:
#        reservations:
#          devices:
#            - driver: nvidia
#              device_ids: [ '0' ]
#              capabilities: [ gpu ]
#    entrypoint: [ "python3", "-m", "fastchat.serve.model_worker", "--model-name", 'fastchat-t5-3b-v1.0', "--model-path", "lmsys/fastchat-t5-3b-v1.0", "--worker-address", "http://fastchat-model-worker:21002", "--controller-address", "http://fastchat-controller:21001", "--host", "0.0.0.0", "--port", "21002", "--num-gpus", "1" ]
  chatglm-model-worker:
    volumes:
      - huggingface:/root/.cache/huggingface
    environment:
      FASTCHAT_CONTROLLER_URL: http://fastchat-controller:21001
    image: fastchat:latest
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: [ '0' ]
              capabilities: [ gpu ]
    entrypoint: [ "python3", "-m", "fastchat.serve.model_worker", "--model-name", 'chatglm-6b', "--model-path", "THUDM/chatglm-6b", "--worker-address", "http://chatglm-model-worker:21003", "--controller-address", "http://fastchat-controller:21001", "--host", "0.0.0.0", "--port", "21003", "--num-gpus", "1" ]
  stablelm-model-worker:
    volumes:
      - huggingface:/root/.cache/huggingface
    environment:
      FASTCHAT_CONTROLLER_URL: http://fastchat-controller:21001
    image: fastchat:latest
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: [ '1', '3' ]
              capabilities: [ gpu ]
    entrypoint: [ "python3", "-m", "fastchat.serve.model_worker", "--model-name", 'stablelm-tuned-alpha-7b', "--model-path", "stabilityai/stablelm-tuned-alpha-7b", "--worker-address", "http://stablelm-model-worker:21004", "--controller-address", "http://fastchat-controller:21001", "--host", "0.0.0.0", "--port", "21004", "--num-gpus", "2" ]
  vicuna-model-worker:
    volumes:
      - huggingface:/root/.cache/huggingface
    environment:
      FASTCHAT_CONTROLLER_URL: http://fastchat-controller:21001
    image: fastchat:latest
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: [ '2', '3' ]
              capabilities: [ gpu ]
    entrypoint: [ "python3", "-m", "fastchat.serve.model_worker", "--model-name", 'vicuna-7b', "--model-path", "/root/.cache/huggingface/models/vicuna-7b", "--worker-address", "http://vicuna-model-worker:21005", "--controller-address", "http://fastchat-controller:21001", "--host", "0.0.0.0", "--port", "21005", "--num-gpus", "2" ]
  fastchat-api-server:
    environment:
      FASTCHAT_CONTROLLER_URL: http://fastchat-controller:21001
    image: fastchat:latest
    ports:
      - "8000:8000"
    entrypoint: [ "python3", "-m", "fastchat.serve.openai_api_server", "--host", "0.0.0.0", "--port", "8000", "--controller-address", "http://fastchat-controller:21001" ]
volumes:
  huggingface:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /data2/docker/volumes/docker_huggingface/_data/